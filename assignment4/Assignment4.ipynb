{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Pytorch Segmentation\n",
    "For this assignment, we're goining to use Deep Learning for a new task: semantic segmentation , instead of classification we've been doing. We will also use some common techniques in Deep Learning like pretraining."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short recap of  semantic segmentation\n",
    "The goal of semantic segmentation is to classify each pixel of the image to a corresponding class of what the pixel represent. One major difference between semantic segmentation and classification is that for semantic segmentation, model output a label for each pixel instead of a single label for the whole image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMP Facade Database and Visualize Samples\n",
    "In this assignment, we use a new dataset named: CMP Facade Database for semantic segmentation. This dataset is made up with 606 rectified images of the facade of various buildings. The facades are from different cities arount the world with different architectural styles.\n",
    "\n",
    "CMP Facade DB include 12 semantic classes:\n",
    "\n",
    "* facade \n",
    "* molding\n",
    "* cornice\n",
    "* pillar\n",
    "* window\n",
    "* door\n",
    "* sill\n",
    "* blind\n",
    "* balcony\n",
    "* shop\n",
    "* deco\n",
    "* background\n",
    "\n",
    "In this assignment, we should use a model to classify each pixel in images to one of these 12 classes.\n",
    "\n",
    "For more detail about CMP Facade Dataset, if you are intereseted, please check: https://cmp.felk.cvut.cz/~tylecr1/facade/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import PI\n",
    "\n",
    "idxs = [1, 2, 5, 6, 7, 8]\n",
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(12, 8))\n",
    "for i, idx in enumerate(idxs):\n",
    "    pic = plt.imread(\"dataset/base/cmp_b000{}.jpg\".format(idx))\n",
    "    label = plt.imread(\"dataset/base/cmp_b000{}.png\".format(idx), format=\"PNG\")\n",
    "\n",
    "    axes[0][i].axis('off')\n",
    "    axes[0][i].imshow(pic)\n",
    "    axes[0][i].set_title(\"Raw Image\")\n",
    "\n",
    "    axes[1][i].imshow(label)\n",
    "    axes[1][i].axis('off')\n",
    "    axes[1][i].set_title(\"Ground Truth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "In semantic segmentations, we will average pixel-wise accuracy and IoU to benchmark semantic segmentation methods. Here we provide, the code for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _hist(pred, gt, n_class):\n",
    "#     mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(\n",
    "        n_class * gt.astype(int) +\n",
    "        pred, minlength=n_class ** 2\n",
    "    ).reshape(n_class, n_class)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def metrics(preds, gts, n_class):\n",
    "    hist = np.zeros((n_class, n_class))\n",
    "    for pred, gt in zip(preds, gts):\n",
    "        hist += _hist(pred.flatten(), gt.flatten(), n_class)\n",
    "    acc = np.diag(hist).sum() / hist.sum()\n",
    "    iou = np.diag(hist) / (\n",
    "        hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist)\n",
    "    )\n",
    "    mean_iou = np.nanmean(iou)\n",
    "    return acc, mean_iou"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset & Dataloader and Set Up Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import PIL\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import os.path as osp\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def get_full_list(\n",
    "    root_dir,\n",
    "    base_dir=\"base\",\n",
    "    extended_dir=\"extended\",\n",
    "):\n",
    "    data_list = []\n",
    "    for name in [base_dir, extended_dir]:\n",
    "        data_dir = osp.join(\n",
    "            root_dir, name\n",
    "        )\n",
    "        data_list += sorted(\n",
    "            osp.join(data_dir, img_name) for img_name in\n",
    "            filter(\n",
    "                lambda x: x[-4:] == '.jpg',\n",
    "                os.listdir(data_dir)\n",
    "            )\n",
    "        )\n",
    "    return data_list\n",
    "\n",
    "class CMP_Facade_DB(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_list\n",
    "    ):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "                \n",
    "    def __getitem__(self, i):\n",
    "        # input and target images\n",
    "        in_name = self.data_list[i]\n",
    "        gt_name = self.data_list[i].replace('.jpg','.png')\n",
    "    \n",
    "        # process the images\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        transf_img = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        \n",
    "        in_image = Image.open(in_name).convert('RGB')\n",
    "        gt_label = Image.open(gt_name)\n",
    "        w, h = in_image.size\n",
    "        gt_label = np.frombuffer(gt_label.tobytes(), dtype=np.ubyte).reshape((h, w))\n",
    "        \n",
    "        in_image = cv2.resize(np.array(in_image), None, fx=0.5, fy=0.5, interpolation = cv2.INTER_NEAREST)\n",
    "        gt_label = cv2.resize(np.array(gt_label), None, fx=0.5, fy=0.5, interpolation = cv2.INTER_NEAREST)\n",
    "        \n",
    "        in_image = transf_img(in_image)\n",
    "        gt_label = torch.tensor(gt_label)\n",
    "        gt_label = (gt_label).long() - 1\n",
    "\n",
    "        return in_image, gt_label\n",
    "    \n",
    "    def revert_input(self, img, label):\n",
    "        img = np.transpose(img.cpu().numpy(), (1, 2, 0))\n",
    "        std_img = np.array([0.229, 0.224, 0.225]).reshape((1, 1, -1))\n",
    "        mean_img = np.array([0.485, 0.456, 0.406]).reshape((1, 1, -1))\n",
    "        img *= std_img\n",
    "        img += mean_img\n",
    "        label = label.cpu().numpy()\n",
    "        return img, label + 1\n",
    "\n",
    "\n",
    "TRAIN_SIZE = 500\n",
    "VAL_SIZE = 30\n",
    "TEST_SIZE = 70\n",
    "full_data_list = get_full_list(\"dataset\")\n",
    "\n",
    "train_data_set = CMP_Facade_DB(full_data_list[: TRAIN_SIZE])\n",
    "val_data_set = CMP_Facade_DB(full_data_list[TRAIN_SIZE: TRAIN_SIZE + VAL_SIZE])\n",
    "test_data_set = CMP_Facade_DB(full_data_list[TRAIN_SIZE + VAL_SIZE:])\n",
    "\n",
    "print(\"Training Set Size:\", len(train_data_set))\n",
    "print(\"Validation Set Size:\", len(val_data_set))\n",
    "print(\"Test Set Size:\", len(test_data_set))\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data_set, batch_size=1, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data_set, batch_size=1, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data_set, batch_size=1, shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Convolutional Networks for Semantic Segmentation\n",
    "\n",
    "Here we are going to explore the classical work: \"Fully Convolutional Networks for Semantic Segmentation\"(FCN).\n",
    "\n",
    "In FCN, the model uses the Transpose Convolution layers, which we've already learned during the lecture, to recover high resolution feature maps. For the overall introduction of Transpose Convolution and Fully Convolutional Networks, please review the lecture recording and lecture slides on Canvas(Lecture 10).\n",
    "\n",
    "Here we do not cover all the details in FCN. Please check the original paper: https://arxiv.org/pdf/1411.4038.pdf for more details.\n",
    "\n",
    "Besides of transpose Convolution, there are also some differences compared with the models we've been working on:\n",
    "\n",
    "* Use 1x1 Convolution to replace fully connected layers to output score for each class.\n",
    "* Use skip connection to combine high-level feature and local feature."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Naive FCN: FCN-32s (30%)\n",
    "\n",
    "In this section, we first try to implement simple version of FCN without skip connection (i.e., FCN-32s) with VGG-16 as the backbone. \n",
    "\n",
    "Compared with VGG-16, FCN-32s \n",
    "* replaces the fully connecteed layers with 1x1 convolution \n",
    "* adds a Transpose Convolution at the end to output dense prediction.\n",
    "\n",
    "Task:\n",
    "1. Complete FCN-32s in the notebook as instructed.\n",
    "2. Train FCN-32s for 10 epochs and record the best model. Visualize the prediction results and report the test accuracy.\n",
    "3. Train FCN-32s for 20 epochs with pretrained VGG-16 weights and record the best model. Visualize the prediction results and report the test accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Complete the FC-32s architecture:\n",
    "\n",
    "The following Conv use kernel size = 3, padding = 1, stride =1 (except for conv1_1 where conv1_1 should use padding = 100). The Max Pool should use \"ceil_mode = True\"\n",
    "\n",
    "* [conv1_1(3,64)-relu] -> [conv1_2(64,64)-relu] -> [maxpool1(2,2)] \n",
    "* [conv2_1(64,128)-relu] -> [conv2_2(128,128)-relu] -> [maxpool2(2,2)]\n",
    "* [conv3_1(128,256)-relu] -> [conv3_2(256,256)-relu] ->[conv3_3(256,256)-relu] ->  [maxpool3(2,2)]\n",
    "* [conv4_1(256,512)-relu] -> [conv4_2(512,512)-relu] ->[conv4_3(512,512)-relu] ->  [maxpool4(2,2)]\n",
    "* [conv5_1(512,512)-relu] -> [conv5_2(512,512)-relu] ->[conv5_3(512,512)-relu] ->  [maxpool5(2,2)]\n",
    "\n",
    "The following Conv use kernel size = 7, stride = 1, padding = 0\n",
    "* [fc6=conv(512, 4096, 7)-relu-dropout2d]\n",
    "\n",
    "The following Conv use kernel size = 1, stride = 1, padding = 0\n",
    "* [fc7=conv1x1(4096, 4096)-relu-dropout2d]\n",
    "* [score=conv1x1(4096, num_classes)]\n",
    "\n",
    "The transpose convolution: kernal size = 64, stride = 32, bias = False\n",
    "* [transpose_conv(n_class, n_class)]\n",
    "\n",
    "\n",
    "**Note: The output of the transpose convlution might not have the same shape as the input, take [19: 19 + input_image_width], [19: 19 + input_image_height] for width and height dimension of the output to get the output with the same shape as the input**\n",
    "\n",
    "**It's expected that you model perform very poor in this section**\n",
    "\n",
    "**Try to name the layers use the name provide above to ensure the next section works correctly, and use a new nn.RELU() for each activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def get_upsampling_weight(in_channels, out_channels, kernel_size):\n",
    "    \"\"\"Make a 2D bilinear kernel suitable for upsampling\"\"\"\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:kernel_size, :kernel_size]\n",
    "    filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "           (1 - abs(og[1] - center) / factor)\n",
    "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n",
    "                      dtype=np.float64)\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return torch.from_numpy(weight).float()\n",
    "\n",
    "class FCN32s(nn.Module):\n",
    "    def __init__(self, n_class=12):\n",
    "        super(FCN32s, self).__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 m.weight.data.zero_()\n",
    "#                 if m.bias is not None:\n",
    "#                     m.bias.data.zero_()\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                assert m.kernel_size[0] == m.kernel_size[1]\n",
    "                initial_weight = get_upsampling_weight(\n",
    "                    m.in_channels, m.out_channels, m.kernel_size[0])\n",
    "                m.weight.data.copy_(initial_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        ################################################################################\n",
    "        # TODO: Implement the forward pass for FCN32s.                                 #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "    \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "        \n",
    "        return h\n",
    "\n",
    "    \n",
    "    def copy_params_from_vgg16(self, vgg16):\n",
    "        features = [\n",
    "            self.conv1_1, self.relu1_1,\n",
    "            self.conv1_2, self.relu1_2,\n",
    "            self.pool1,\n",
    "            self.conv2_1, self.relu2_1,\n",
    "            self.conv2_2, self.relu2_2,\n",
    "            self.pool2,\n",
    "            self.conv3_1, self.relu3_1,\n",
    "            self.conv3_2, self.relu3_2,\n",
    "            self.conv3_3, self.relu3_3,\n",
    "            self.pool3,\n",
    "            self.conv4_1, self.relu4_1,\n",
    "            self.conv4_2, self.relu4_2,\n",
    "            self.conv4_3, self.relu4_3,\n",
    "            self.pool4,\n",
    "            self.conv5_1, self.relu5_1,\n",
    "            self.conv5_2, self.relu5_2,\n",
    "            self.conv5_3, self.relu5_3,\n",
    "            self.pool5,\n",
    "        ]\n",
    "        for l1, l2 in zip(vgg16.features, features):\n",
    "            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n",
    "                assert l1.weight.size() == l2.weight.size()\n",
    "                assert l1.bias.size() == l2.bias.size()\n",
    "                l2.weight.data = l1.weight.data\n",
    "                l2.bias.data = l1.bias.data\n",
    "        for i, name in zip([0, 3], ['fc6', 'fc7']):\n",
    "            l1 = vgg16.classifier[i]\n",
    "            l2 = getattr(self, name)\n",
    "            l2.weight.data = l1.weight.data.view(l2.weight.size())\n",
    "            l2.bias.data = l1.bias.data.view(l2.bias.size())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  Train FCN-32s from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def Evaluate(\n",
    "    val_loader,\n",
    "    model,\n",
    "    current_best,\n",
    "    n_class=12\n",
    "):\n",
    "    val_loss = 0\n",
    "    visualizations = []\n",
    "    preds, gts = [], []\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        with torch.no_grad():\n",
    "            score = model(data)\n",
    "\n",
    "        pred = score.max(1)[1].cpu().numpy()\n",
    "        gt = target.cpu().numpy()\n",
    "        preds.append(pred)\n",
    "        gts.append(gt)\n",
    "\n",
    "    avg_acc, mean_iou = metrics(\n",
    "        preds, gts, n_class)\n",
    "\n",
    "    if mean_iou > current_best[\"IoU\"]:\n",
    "        current_best[\"IoU\"] = mean_iou\n",
    "        current_best[\"model\"] = copy.deepcopy(model)\n",
    "\n",
    "    return avg_acc, mean_iou, current_best\n",
    "\n",
    "def Train(\n",
    "    model,\n",
    "    loss_func,\n",
    "    optim,\n",
    "    scheduler,\n",
    "    epochs,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    display_interval = 100\n",
    "):\n",
    "\n",
    "    current_best = {\n",
    "        \"IoU\": 0,\n",
    "        \"model\": model\n",
    "    }\n",
    "    avg_acc, mean_iou, current_best = Evaluate(\n",
    "        val_loader,\n",
    "        model,\n",
    "        current_best\n",
    "    )\n",
    "    \n",
    "    print(\"Init Model\")\n",
    "    print(\"Avg Acc: {:.4}, Mean IoU: {:.4}\".format(\n",
    "        avg_acc, mean_iou\n",
    "    ))\n",
    "    for i in range(epochs):\n",
    "        print(\"Epochs: {}\".format(i))\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(\"cuda:0\"), target.to(\"cuda:0\")\n",
    "            optim.zero_grad()\n",
    "\n",
    "            score = model(data)\n",
    "            loss = loss_func(score, target.squeeze(1))\n",
    "            loss_data = loss.item()\n",
    "            if np.isnan(loss_data):\n",
    "                raise ValueError('loss is nan while training')\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.item()\n",
    "            # if batch_idx % display_interval == 0 and batch_idx != 0:\n",
    "            #     print(\"{} / {}, Current Avg Loss:{:.4}\".format(\n",
    "            #         batch_idx, len(train_loader), total_loss / (batch_idx + 1)\n",
    "            #     ))\n",
    "            \n",
    "        \n",
    "        total_loss /= len(train_loader)\n",
    "        model.eval()\n",
    "        avg_acc, mean_iou, current_best = Evaluate(\n",
    "            val_loader,\n",
    "            model,\n",
    "            current_best\n",
    "        )\n",
    "        scheduler.step(total_loss)\n",
    "        print(\"Avg Loss: {:.4}, Avg Acc: {:.4}, Mean IoU: {:.4}\".format(\n",
    "            total_loss, avg_acc, mean_iou\n",
    "        ))\n",
    "    \n",
    "    test_acc, test_iou, current_best = Evaluate(\n",
    "        val_loader,\n",
    "        current_best[\"model\"],\n",
    "        current_best\n",
    "    )\n",
    "    print(\"Test Acc: {:.4}, Test Mean IoU: {:.4}\".format(\n",
    "        test_acc, test_iou\n",
    "    ))\n",
    "    return current_best[\"model\"]\n",
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def Trainer(model, \n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            test_loader,\n",
    "            num_epochs=25, \n",
    "            ):\n",
    "    # define optimizer\n",
    "    lr = 1e-4\n",
    "    weight_decay = 2e-5\n",
    "    optim = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "    \n",
    "    # define learning rate schedule\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optim, 'min', patience=3,\n",
    "        min_lr=1e-10, verbose=True\n",
    "    )\n",
    "    \n",
    "    # define loss function\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_model = Train(\n",
    "        model,\n",
    "        loss_func,\n",
    "        optim,\n",
    "        scheduler,\n",
    "        num_epochs,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader\n",
    "    )\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model32 = FCN32s(n_class=12)\n",
    "model32.to(device)\n",
    "\n",
    "best_model = Trainer(\n",
    "    model32,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Output\n",
    "In this section, we visualize several model outputs to see how our model actually perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(model, test_loader):\n",
    "    idxs = [1, 2, 5, 6, 7, 8]\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=6, figsize=(12, 8))\n",
    "    model.eval()\n",
    "    for i, idx in enumerate(idxs):\n",
    "        img, label = test_loader.dataset[idx]\n",
    "        \n",
    "        pred = model(img.unsqueeze(0).to(device)) \n",
    "        pred = (pred.max(1)[1] + 1).squeeze(0).cpu().numpy()\n",
    "        \n",
    "        img, label = test_loader.dataset.revert_input(img, label)\n",
    "        \n",
    "        axes[0][i].axis('off')\n",
    "        axes[0][i].imshow(img)\n",
    "        axes[0][i].set_title(\"Raw Image\")\n",
    "\n",
    "        axes[1][i].imshow(label)\n",
    "        axes[1][i].axis('off')\n",
    "        axes[1][i].set_title(\"Ground Truth\")\n",
    "\n",
    "        axes[2][i].imshow(pred)\n",
    "        axes[2][i].axis('off')\n",
    "        axes[2][i].set_title(\"prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(best_model, test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Train FCN-32s with the pretrained VGG16 weights\n",
    "\n",
    "In the previous section, we use the random initalized weights to train FCN-32S from scrath. We can see that it perform poorly. In this section, we utilize the feature from pretrained model(In our case, we use VGG-16) to help us get a better result.\n",
    "\n",
    "**The model should achieve at least 50% Test Accuracy, 0.3 Mean IoU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "model32_pretrain = FCN32s(n_class=12)\n",
    "model32_pretrain.copy_params_from_vgg16(vgg16)\n",
    "model32_pretrain.to(device)\n",
    "\n",
    "best_model_pretrain = Trainer(\n",
    "    model32_pretrain,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    num_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(best_model_pretrain, test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Skip Connection: FCN-8s(40%)\n",
    "\n",
    "In this section, we explore with another technique introduced in FCN paper: Skip Connection.\n",
    "\n",
    "Task: Read the paper and understand the skip connection, then \n",
    "1. Complete FCN-8s in the notebook as instructed.\n",
    "2. Train the network for 20 epochs with pretrained VGG-16 weights and record the best model. Visualize the prediction results and report the test accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide the structure of FCN-8s, the variant of FCN with skip connections.\n",
    "\n",
    "FCN-8s architecture:\n",
    "\n",
    "The following Conv use kernel size = 3, padding = 1, stride =1(except conv1_1. conv1_1 should use padding = 100)\n",
    "The Max Pool should use \"ceil_mode = True\"\n",
    "\n",
    " **As you can see, the structure of this part is the same as FCN-32s**\n",
    "\n",
    "* [conv1_1(3,64)-relu] -> [conv1_2(64,64)-relu] -> [maxpool1(2,2)] \n",
    "* [conv2_1(64,128)-relu] -> [conv2_2(128,128)-relu] -> [maxpool2(2,2)]\n",
    "* [conv3_1(128,256)-relu] -> [conv3_2(256,256)-relu] ->[conv3_3(256,256)-relu] -> [maxpool3(2,2)]\n",
    "* [conv4_1(256,512)-relu] -> [conv4_2(512,512)-relu] ->[conv4_3(512,512)-relu] -> [maxpool3(2,2)]\n",
    "* [conv5_1(512,512)-relu] -> [conv5_2(512,512)-relu] ->[conv5_3(512,512)-relu] -> [maxpool3(2,2)]\n",
    "\n",
    "The following Conv use kernel size = 7, stride = 1, padding = 0\n",
    "* [fc6=conv(512, 4096, 7)-relu-dropout2d]\n",
    "\n",
    "The following Conv use kernel size = 1, stride = 1, padding = 0\n",
    "* [fc7=conv1x1(4096, 4096)-relu-dropout2d]\n",
    "* [score=conv1x1(4096, num_classes)]\n",
    "\n",
    "The Additional Score Pool use kernel size = 1, stride = 1, padding = 0\n",
    "* [score_pool_3 =conv1x1(256, num_classes)]\n",
    "* [score_pool_4 =conv1x1(512, num_classes)]\n",
    "\n",
    "The transpose convolution: kernel size = 4, stride = 2, bias = False\n",
    "* [upscore1 = transpose_conv(n_class, n_class)]\n",
    "\n",
    "The transpose convolution: kernel size = 4, stride = 2, bias = False\n",
    "* [upscore2 = transpose_conv(n_class, n_class)]\n",
    "\n",
    "The transpose convolution: kernel size = 16, stride = 8, bias = False\n",
    "* [upscore3 = transpose_conv(n_class, n_class)]\n",
    "\n",
    "Different from FCN-32s which has only single path from input to output, there are multiple data path from input to output in FCN-8s.\n",
    "\n",
    "The following graph is from original FCN paper, you can also find the graph there.\n",
    "\n",
    "![\"Architecture Graph\"](arch.png)\n",
    "\"Layers are shown as grids that reveal relative spatial coarseness. Only pooling and prediction layers are shown; intermediate convolution layers (including converted fully connected layers) are omitted. \" ---- FCN\n",
    "\n",
    "Detailed path specification:\n",
    "\n",
    "* score_pool_3\n",
    "    * input: output from layer \"pool3\"\n",
    "    * take [9: 9 + upscore2_width], [9: 9 + upscore2_height]\n",
    "    \n",
    "* score_pool_4,\n",
    "    * input: output from layer \"pool4\"\n",
    "    * take [5: 5 + upscore1_width], [5: 5 + upscore1_height]\n",
    "\n",
    "\n",
    "* upscore1\n",
    "    * input: output from layer \"score\"\n",
    "\n",
    "* upscore2:\n",
    "    * input: output from layer \"score_pool_4\" + output from layer \"upscore1\"\n",
    "\n",
    "* upscore3:\n",
    "    * input: output from layer \"score_pool_3\" + output from layer \"upscore2\"\n",
    "    * take [31: 31 + input_image_width], [31: 31 + input_image_height]\n",
    "\n",
    "\n",
    "**The model should achieve at least 40% Mean IoU**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FCN8s(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class=12):\n",
    "        super(FCN8s, self).__init__()\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            # if isinstance(m, nn.Conv2d):\n",
    "            #     m.weight.data.zero_()\n",
    "            #     if m.bias is not None:\n",
    "            #         m.bias.data.zero_()\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                assert m.kernel_size[0] == m.kernel_size[1]\n",
    "                initial_weight = get_upsampling_weight(\n",
    "                    m.in_channels, m.out_channels, m.kernel_size[0])\n",
    "                m.weight.data.copy_(initial_weight)\n",
    "\n",
    "                \n",
    "    def forward(self, x):\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        return h\n",
    "\n",
    "    def copy_params_from_vgg16(self, vgg16):\n",
    "        features = [\n",
    "            self.conv1_1, self.relu1_1,\n",
    "            self.conv1_2, self.relu1_2,\n",
    "            self.pool1,\n",
    "            self.conv2_1, self.relu2_1,\n",
    "            self.conv2_2, self.relu2_2,\n",
    "            self.pool2,\n",
    "            self.conv3_1, self.relu3_1,\n",
    "            self.conv3_2, self.relu3_2,\n",
    "            self.conv3_3, self.relu3_3,\n",
    "            self.pool3,\n",
    "            self.conv4_1, self.relu4_1,\n",
    "            self.conv4_2, self.relu4_2,\n",
    "            self.conv4_3, self.relu4_3,\n",
    "            self.pool4,\n",
    "            self.conv5_1, self.relu5_1,\n",
    "            self.conv5_2, self.relu5_2,\n",
    "            self.conv5_3, self.relu5_3,\n",
    "            self.pool5,\n",
    "        ]\n",
    "        for l1, l2 in zip(vgg16.features, features):\n",
    "            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n",
    "                assert l1.weight.size() == l2.weight.size()\n",
    "                assert l1.bias.size() == l2.bias.size()\n",
    "                l2.weight.data.copy_(l1.weight.data)\n",
    "                l2.bias.data.copy_(l1.bias.data)\n",
    "        for i, name in zip([0, 3], ['fc6', 'fc7']):\n",
    "            l1 = vgg16.classifier[i]\n",
    "            l2 = getattr(self, name)\n",
    "            l2.weight.data.copy_(l1.weight.data.view(l2.weight.size()))\n",
    "            l2.bias.data.copy_(l1.bias.data.view(l2.bias.size()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "model8 = FCN8s(n_class=12)\n",
    "model8.copy_params_from_vgg16(vgg16)\n",
    "model8.to(device)\n",
    "\n",
    "best_model_fcn8s = Trainer(\n",
    "    model8,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader, \n",
    "    num_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(best_model_fcn8s, test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Questions(30%):\n",
    "\n",
    "#### Question 1: Compare the FCN-32s training from scratch with the FCN-32s with pretrained weights? What do you observe? Does pretrained weights help? Why? Please be as specific as possible.\n",
    "\n",
    "### Your Answer:\n",
    "\n",
    "\n",
    "#### Question 2: Compare the performance and visualization of FCN-32s and FCN-8s (both with pretrained weights). What do you observe? Which performs better? Why? Please be as specific as possible.\n",
    "\n",
    "### Your Answer:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
